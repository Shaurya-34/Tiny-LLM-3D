# ğŸ§  Tiny-LLM-3D  
A minimal AI-powered 3D scene generator for Blender â€” converts natural language prompts into 3D object renders.

---

## âœ¨ Features
- ğŸ§© Uses a fine-tuned LLM to parse text prompts â†’ structured JSON actions  
- ğŸ§± Executes actions safely inside Blender  
- ğŸ¨ Automatically renders cinematic images  
- ğŸ§  No cloud dependency â€” runs fully locally  

---

## ğŸ“ Project Structure
```
Tiny-LLM-3D/
â”œâ”€â”€ demo_prompt_to_render.py     # main script to run prompt â†’ render
â”œâ”€â”€ worker/
â”‚   â”œâ”€â”€ infer.py                 # text â†’ JSON inference logic
â”‚   â”œâ”€â”€ blender_executor.py      # executes JSON inside Blender
â”‚   â”œâ”€â”€ blender_wrapper.py       # bpy-based scene control
â”‚   â”œâ”€â”€ inbox_watcher.py         # watches for new jobs
â”‚   â”œâ”€â”€ action_schema.json       # JSON schema validation
â”œâ”€â”€ Models/
â”‚   â””â”€â”€ json_gen_model/          # fine-tuned model folder (add README here)
â”œâ”€â”€ outbox/                      # rendered output images
â””â”€â”€ requirements.txt
```

---

## âš™ï¸ Setup

1. **Clone the repo**
   ```bash
   git clone https://github.com/Shaurya-34/Tiny-LLM-3D.git
   cd Tiny-LLM-3D
   ```

2. **Create & activate virtual environment**
   ```bash
   python -m venv .venv
   .venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Verify Blender is installed**
   ```bash
   blender --version
   ```

---

## ğŸ§  Model Setup

This project uses a **local fine-tuned model** to convert natural language prompts into structured JSON.  
The model is **not included** in this repository to keep it lightweight.

### ğŸª¶ Option 1: Use your own fine-tuned model
Place your Hugging Face or fine-tuned model inside:
```
Tiny-LLM-3D/Models/json_gen_model/
```
Ensure it contains:
```
config.json
pytorch_model.bin
tokenizer.json
tokenizer_config.json
```

Then run:
```bash
python worker/infer.py "add a red cube at (0,0,0)" --model-dir "Models/json_gen_model"
```

### âš™ï¸ Option 2: Use rule-based fallback
If you don't have a model, Tiny-LLM-3D will **automatically switch** to a built-in rule-based mode that still generates valid JSON for simple prompts.

Youâ€™ll see:
```
Model load failed or not available; will use rule-based fallback.
```

Thatâ€™s normal and safe â€” this fallback ensures the demo still runs fully offline.

---

## ğŸš€ Run the Demo

> ğŸ’¡ **Tip:** Before running any `infer.py` commands, make sure the inbox watcher is active.  
Start it in a separate terminal:
```bash
python worker/inbox_watcher.py --blender "C:/Path/to/blender.exe" --inbox worker/inbox --worker worker/blender_executor.py
```

Generate action JSON:
```bash
python worker/infer.py "add a red cube at (0,0,0)" --model-dir "Models/json_gen_model"
```

Render it in Blender:
```bash
python demo_prompt_to_render.py
```

Outputs will appear in `outbox/`.

---

## ğŸ–¼ï¸ Example Outputs

| Prompt | Render |
|--------|---------|
| â€œadd a shiny yellow sphereâ€ | ![](examples/photo1.png) |
| â€œadd a green torus at (1,2,0)â€ | ![](examples/photo2.png) |
| â€œadd a blue sphere at (-2,0,0)â€ | ![](examples/photo3.png) |

---

## ğŸ“¦ Requirements
```
torch
transformers
jsonschema
bpy
tqdm
```

---

## ğŸ§± Credits
Created by [Shaurya-34](https://github.com/Shaurya-34)  
Inspired by the goal of bridging LLMs with 3D creative workflows.
